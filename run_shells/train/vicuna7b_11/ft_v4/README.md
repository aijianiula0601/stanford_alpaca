# 说明

加载vicuna-7b-1.1来训练

地址：https://huggingface.co/eachadea/vicuna-7b-1.1

# 数据集

ft biglive线上数据，这里的数据，永强吧露馅的修改了。

biglive: /mnt/cephfs/pangyongqiang/proj/LLM/chatgpt_goof/revized_proof_data_8000_1_key.json

# 结果

效果不错，跟线上Gpt相似



# 原始模型存在问题

1.回复太长

2.连续反问

3.后尾出现:)这个笑脸符号概率大问题



# 解决方案

- 尝试实验方案一：

        重新训练模型，只训练一个opoch，让模型学回复的风格，不过分训练。
        训练完个人体验(ft_vicuna7b_v4_v2.sh)：
        1.像之前模型那样特别长的回复没遇到过
        2.回复结尾出现:)这个概率挺高（数据集中有很多回复包含 :) ）
        3.有遇到同一个回复中两个连续反问的情况（缓解了之前那种很不舒服的被反问感觉），但没之前模型那么严重的连续多个反问，（需重点关注优化的点）

- 尝试实验方案二：
        
        修正训练数据
        
        1.程序剔除一些回复过长的回复。
        2.程序删除 :) 笑脸
        训练完个人体验(ft_vicuna7b_v4_v3.sh)：
        1.像之前模型那样特别长的回复没遇到过
        2.回复结尾没遇到出现 :) 的情况，说明删除:)的推断是正确的。
        3.有遇到同一个回复中两个连续反问的情况（缓解了之前那种很不舒服的被反问感觉），但没之前模型那么严重的连续多个反问，（需重点关注优化的点）


- 修复连续反问方案三
  
	    做法：过滤连续反问的训练数据
        训练完个人体验(ft_vicuna7b_v4_v4.sh)：
        体验过程未遇到连续反问的情况
        存在问题：
        1.回答较为正式
        2.在回答后大概率会提问题，个人认为无需每个回答最后都提问（线上chatgpt也如此），显得太没个性属于服务型人格。
               准备尝试解决的方案：
            1)修改不同prompt来实验。
            2)混合其他数据集改善。


- 修复方案效果总结
  
        三个方案都解决了相应的问题，所以目前最后的方案是方案三，建议后面只ft一个opoch就行，原因有：
  
        1.ft多会过拟合训练数据，线上的Biglive聊天数据本来格式重复读就高
        2.避免偏离了原始模型的逻辑能力，ft多的话，个人推测原始模型的逻辑能力会退化。